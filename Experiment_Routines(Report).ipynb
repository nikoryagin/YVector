{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment Routines(Report).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rCvEv8pPX4KL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "А) Какая задача решалась?\n",
        "\n",
        "Данная статья ставила перед собой целью улучшить существующие подходы создания эмбеддингов звуковых дорожек напрямую из wave form. Результаты сравниваются на задаче верификации - о ней подробнее ниже.\n",
        "\n",
        "Б) В чём основная идея метода и в чём её отличие от других решений? Пишите своими словами.\n",
        "\n",
        "Основная идея метода - обработка информации о звукой дорожки непосредственно из звука - обычно используют некоторую предобработку волн для приведения ее в другой формат, будь это МЕЛ спектрограмма или что-либо еще. Эта идея не нова - уже существующие модели работающие напрямую со звуком показывают неплохой результат. Все такие модели используют первым слоем свертку с каким-то одним фиксированным набором параметров - размером ядра, шагом свертки и т.п. Такой подход не позволяет модели работать с волнами разной частоты. Авторы Yvector предлагают такое решение этой проблемы - запустить несколько сверток параллельно, каждая из которых будет обрабатывать волны своей частотности - низкой, высокой или средней.\n",
        "\n",
        "В) Какой эксперимент ставился? Какие получились результаты и как их можно интерпретировать?\n",
        "\n",
        "Г) Как можно использовать полученный результат? Удалось ли приблизиться к цифрам из статьи? Какие есть перспективы для развития?\n",
        "\n",
        "Про эксперимент чуть ниже.\n"
      ],
      "metadata": {
        "id": "D1i8cbx-c0Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model weights and testing dataset"
      ],
      "metadata": {
        "id": "9LqgfeF6RW8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nikoryagin/YVector.git"
      ],
      "metadata": {
        "id": "aktToSoGFQyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd YVector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrdpVh2pFS5N",
        "outputId": "083dd0ee-9aae-4079-8767-2011fcaee44f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/YVector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih8JOzB5Dmp2"
      },
      "outputs": [],
      "source": [
        "!wget https://us.openslr.org/resources/12/test-clean.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf test-clean.tar.gz"
      ],
      "metadata": {
        "id": "KB__ekXpFHGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"*.txt\" -type f -delete"
      ],
      "metadata": {
        "id": "qJ9mnf0oFLeW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from yvector import YVectorModel\n",
        "model = YVectorModel().to('cuda')\n",
        "checkpoint = torch.load('/content/drive/MyDrive/vk/vk18.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMLG8JjtFbyI",
        "outputId": "0f34adce-c0f2-4e37-ba04-93b862029d33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "xvnfYEm0Rpp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы повторить результаты эксперимента из статьи, я скачал тестовый датасет. В силу ограниченности ресурсов, тестовый(как и обучающий) датасет отличается от того, который использовали в статье. В чем заключается эксперимент: будем решать задачу верификации по голосу -- по двум записям с голосом, определить, принадлежит ли голос одному и тому же человеку. \n",
        "Из исходного тестового датасета будем тысячу раз генерировать по две пары звуковых дорожек - в одной паре обе дорожки принадлежат одному человеку, в другой - разным. Затем, каждая дорожка из обоих пар прогоняется через модель, таким образом, получаются эмбеддинги дорожек. В каждой паре будем считать косинус угла между эмбеддингами - мера \"похожести\" эмбеддингов.\n",
        "На основе полученных значений косинусов и тому, принадлежит ли этот косинус паре с голосами одного человека(класс 1) или двух(класс 0), мы считаем метрики EER и minDCF - они в некотором смысле показывают, насколько в приниципе модель научилась разделять два класса. "
      ],
      "metadata": {
        "id": "BRotRsybRuZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import pathlib\n",
        "import os, torchaudio, random\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "speakers_dirs = os.listdir('LibriSpeech/test-clean')\n",
        "root =  Path('LibriSpeech/test-clean')\n",
        "positive_similarity = []\n",
        "negative_similarity = []\n",
        "model.eval()\n",
        "model = model.to('cuda')\n",
        "for i in range(1000):\n",
        "  # Generate 2 random audiofile from (i % num_speakers)th speaker\n",
        "  anc_spk_dir = root / speakers_dirs[i % len(speakers_dirs)]\n",
        "  tmp_dirs = os.listdir(anc_spk_dir)\n",
        "  tmp_dir = anc_spk_dir / tmp_dirs[random.randint(0, len(tmp_dirs) - 1)]\n",
        "  tmp_files = os.listdir(tmp_dir)\n",
        "  anchor_wave, _ = torchaudio.load(tmp_dir / tmp_files[random.randint(0, len(tmp_files) - 1)])\n",
        "  pair_wave_true, _ = torchaudio.load(tmp_dir / tmp_files[random.randint(0, len(tmp_files) - 1)])\n",
        "\n",
        "  # Generate random audiofile from other speaker\n",
        "  pair_false_spk_dir = root / speakers_dirs[random.randint(0, len(speakers_dirs) - 1)]\n",
        "  while pair_false_spk_dir == anc_spk_dir:\n",
        "      pair_false_spk_dir = root / speakers_dirs[random.randint(0, len(speakers_dirs) - 1)]\n",
        "\n",
        "  tmp_dirs = os.listdir(pair_false_spk_dir)\n",
        "  tmp_dir = pair_false_spk_dir / tmp_dirs[random.randint(0, len(tmp_dirs) - 1)]\n",
        "  tmp_files = os.listdir(tmp_dir)\n",
        "  pair_wave_false, _ = torchaudio.load(tmp_dir / tmp_files[random.randint(0, len(tmp_files) - 1)])\n",
        "\n",
        "  # Calculate embeddings for all audiofiles\n",
        "  embed_anchor = model(anchor_wave.unsqueeze(0).to('cuda')).cpu().squeeze()\n",
        "  embed_true = model(pair_wave_true.unsqueeze(0).to('cuda')).cpu().squeeze()\n",
        "  embed_false = model(pair_wave_false.unsqueeze(0).to('cuda')).cpu().squeeze()\n",
        "\n",
        "  # Calculate cosine similarity\n",
        "  positive_similarity += [F.cosine_similarity(embed_anchor, embed_true, dim=0).detach().numpy()]\n",
        "  negative_similarity += [F.cosine_similarity(embed_anchor, embed_false, dim=0).detach().numpy()]"
      ],
      "metadata": {
        "id": "YCBTCw_bFxWZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "rCvEv8pPX4KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from https://github.com/kaldi-asr/kaldi/blob/master/egs/sre08/v1/sid/compute_min_dcf.py\n",
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "from operator import itemgetter\n",
        "\n",
        "def calculate_eer(positive_sim, negative_sim):\n",
        "    target_scores = sorted(positive_sim)\n",
        "    nontarget_scores = sorted(negative_sim)\n",
        "\n",
        "    target_size = len(target_scores)\n",
        "    nontarget_size = len(nontarget_scores)\n",
        "\n",
        "    target_position = 0\n",
        "    for target_position in range(target_size):\n",
        "        nontarget_n = nontarget_size * target_position * 1.0 / target_size\n",
        "        nontarget_position = int(nontarget_size - 1 - nontarget_n)\n",
        "        if nontarget_position < 0:\n",
        "            nontarget_position = 0\n",
        "        if nontarget_scores[nontarget_position] < target_scores[target_position]:\n",
        "            break\n",
        "\n",
        "    threshold = target_scores[target_position]\n",
        "    eer = target_position * 1.0 / target_size\n",
        "\n",
        "    return eer, threshold\n",
        "\n",
        "def ComputeErrorRates(scores, labels):\n",
        "    # Sort the scores from smallest to largest, and also get the corresponding\n",
        "    # indexes of the sorted scores.  We will treat the sorted scores as the\n",
        "    # thresholds at which the the error-rates are evaluated.\n",
        "    sorted_indexes, thresholds = zip(*sorted(\n",
        "        [(index, threshold) for index, threshold in enumerate(scores)],\n",
        "        key=itemgetter(1)))\n",
        "    sorted_labels = []\n",
        "    labels = [labels[i] for i in sorted_indexes]\n",
        "    fnrs = []\n",
        "    fprs = []\n",
        "\n",
        "    # At the end of this loop, fnrs[i] is the number of errors made by\n",
        "    # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n",
        "    # is the total number of times that we have correctly accepted scores\n",
        "    # greater than thresholds[i].\n",
        "    for i in range(0, len(labels)):\n",
        "        if i == 0:\n",
        "            fnrs.append(labels[i])\n",
        "            fprs.append(1 - labels[i])\n",
        "        else:\n",
        "            fnrs.append(fnrs[i-1] + labels[i])\n",
        "            fprs.append(fprs[i-1] + 1 - labels[i])\n",
        "    fnrs_norm = sum(labels)\n",
        "    fprs_norm = len(labels) - fnrs_norm\n",
        "\n",
        "    # Now divide by the total number of false negative errors to\n",
        "    # obtain the false positive rates across all thresholds\n",
        "    fnrs = [x / float(fnrs_norm) for x in fnrs]\n",
        "\n",
        "    # Divide by the total number of corret positives to get the\n",
        "    # true positive rate.  Subtract these quantities from 1 to\n",
        "    # get the false positive rates.\n",
        "    fprs = [1 - x / float(fprs_norm) for x in fprs]\n",
        "    return fnrs, fprs, thresholds\n",
        "\n",
        "# Computes the minimum of the detection cost function.  The comments refer to\n",
        "# equations in Section 3 of the NIST 2016 Speaker Recognition Evaluation Plan.\n",
        "def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n",
        "    min_c_det = float(\"inf\")\n",
        "    min_c_det_threshold = thresholds[0]\n",
        "    for i in range(0, len(fnrs)):\n",
        "        # See Equation (2).  it is a weighted sum of false negative\n",
        "        # and false positive errors.\n",
        "        c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n",
        "        if c_det < min_c_det:\n",
        "            min_c_det = c_det\n",
        "            min_c_det_threshold = thresholds[i]\n",
        "    # See Equations (3) and (4).  Now we normalize the cost.\n",
        "    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
        "    min_dcf = min_c_det / c_def\n",
        "    return min_dcf, min_c_det_threshold\n",
        "\n",
        "def calculate_minDCF(scores, labels, p_target, c_miss, c_fa):\n",
        "\n",
        "    fnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n",
        "    min_dcf, min_c_det_threshold = ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa)\n",
        "    \n",
        "    return min_dcf, min_c_det_threshold"
      ],
      "metadata": {
        "id": "zIDOGmCIF4z_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "qPXOyyKIYCR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_scores = positive_similarity + negative_similarity\n",
        "total_results = [1] * len(positive_similarity) + [0] * len(negative_similarity)\n",
        "\n",
        "min_dcf2, min_c_det_threshold2 = calculate_minDCF(total_scores, total_results, 0.01, 1, 1)\n",
        "min_dcf3, min_c_det_threshold3 = calculate_minDCF(total_scores, total_results, 0.001, 1, 1)\n",
        "\n",
        "print('minDCF:0.01 {0:0.4f},{1:0.4f}'.format(min_dcf2, min_c_det_threshold2))\n",
        "print('minDCF:0.001 :{0:0.4f},{1:0.4f}'.format(min_dcf3, min_c_det_threshold3))\n",
        "\n",
        "# eer\n",
        "positive_similarity = np.array(positive_similarity)\n",
        "negative_similarity = np.array(negative_similarity)\n",
        "\n",
        "eer, threshold = calculate_eer(positive_similarity, negative_similarity)\n",
        "print('eer: {}%'.format(eer * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6B3KBEmF9uz",
        "outputId": "e8318138-3893-4086-efd1-97183675dffa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minDCF:0.01 0.6050,0.6297\n",
            "minDCF:0.001 :0.8250,0.8959\n",
            "eer: 3.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обзор результатов** (Ответ на вопросы В и Г)\n",
        "\n",
        "Значение основной метрики Equal Error Rate(eer) сопоставимо с результатами, опубликованными в статье(хоть я его получил и на гораздо более простом датасете).\n",
        "\n",
        "Хочу сразу заметить несколько моментов:\n",
        "\n",
        "1) Как уже сказано, в виду огранничености ресурсов, я обучал модель на меньшем и более простом датасете и в течение меньшего количества эпох.\n",
        "\n",
        "2) Не все детали архитектуры были раскрыты в статье, так что в некоторых местах пришлось импровизировать(в частности, в подборе некоторых гиперпараметров)\n",
        "\n",
        "Таким образом, я получил достойный результат, сопоставимый с SOTA моделями. Но, опять же, точно сравнить не получится, обычно на датасете, на котором я обучился и тестировался, не решают задачу верификации(этим объясняются танцы с бубнами в коде тестирования). Еще раз отмечу, что в строгих условиях ограниченных ресурсов, получился хороший результат, что говорит о фундаментальной идейной \"правильности\" архитектуры YVector.\n",
        "\n",
        "Полученные результаты можно использовать во всех областях обработки и синтеза звука - значения метрик, полученные для задачи верификации позволяют сделать предположение, что эмбеддинги YVector несут в себе больше информации. В качестве перспектив развития можно предложить исследования в области архитектуры - улучшится ли результат если добавить больше параллельных сверток? Возможны ли другие способы обработки кроме сверток? Self-attention?"
      ],
      "metadata": {
        "id": "ZwL6S3BPpNVG"
      }
    }
  ]
}